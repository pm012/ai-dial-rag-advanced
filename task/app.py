from task._constants import API_KEY
from task.chat.chat_completion_client import DialChatCompletionClient
from task.embeddings.embeddings_client import DialEmbeddingsClient
from task.embeddings.text_processor import TextProcessor, SearchMode
from task.models.conversation import Conversation
from task.models.message import Message
from task.models.role import Role


# Create system prompt with info that it is RAG powered assistant.
# Explain user message structure (firstly will be provided RAG context and the user question).
# Provide instructions that LLM should use RAG Context when answer on User Question, will restrict LLM to answer
# questions that are not related microwave usage, not related to context or out of history scope
SYSTEM_PROMPT = """You are a RAG-powered assistant that assists users with their questions about microwave usage.
            
## Structure of User message:
`RAG CONTEXT` - Retrieved documents relevant to the query.
`USER QUESTION` - The user's actual question.

## Instructions:
- Use information from `RAG CONTEXT` as context when answering the `USER QUESTION`.
- Cite specific sources when using information from the context.
- Answer ONLY based on conversation history and RAG context.
- If no relevant information exists in `RAG CONTEXT` or conversation history, state that you cannot answer the question.
"""

# Provide structured system prompt, with RAG Context and User Question sections.
USER_PROMPT = """##RAG CONTEXT:
{context}


##USER QUESTION: 
{query}"""


# - create embeddings client with 'text-embedding-3-small-1' model
# - create chat completion client
# - create text processor, DB config: {'host': 'localhost','port': 5433,'database': 'vectordb','user': 'postgres','password': 'postgres'}
# ---
# Create method that will run console chat with such steps:
# - get user input from console
# - retrieve context
# - perform augmentation
# - perform generation
# - it should run in `while` loop (since it is console chat)

def main():
    # Initialize clients and processor
    embeddings_client = DialEmbeddingsClient(
        deployment_name='text-embedding-3-small-1',
        api_key=API_KEY
    )
    chat_client = DialChatCompletionClient(
        deployment_name='gpt-4o',
        api_key=API_KEY
    )
    
    db_config = {
        'host': 'localhost',
        'port': 5433,
        'database': 'vectordb',
        'user': 'postgres',
        'password': 'postgres'
    }
    
    text_processor = TextProcessor(
        embeddings_client=embeddings_client,
        db_config=db_config
    )
    
    # Process the microwave manual (optional)
    print("üéØ Microwave RAG Assistant")
    print("=" * 100)
    load_context = input("\nLoad context to VectorDB (y/n)? > ").strip()
    if load_context.lower().strip() in ['y', 'yes']:
        text_processor.process_text_file(
            file_name='task/embeddings/microwave_manual.txt',
            chunk_size=300,
            overlap=40,
            dimensions=1536,
            truncate_table=True
        )
        print("=" * 100)
    
    # Initialize conversation
    conversation = Conversation()
    conversation.add_message(Message(Role.SYSTEM, SYSTEM_PROMPT))
    
    # Main chat loop
    while True:
        # Get user input from console
        user_request = input("\n‚û°Ô∏è ").strip()
        
        if not user_request:
            continue
            
        if user_request.lower().strip() in ['exit', 'quit']:
            print("üëã Goodbye")
            break
        
        # Step 1: Retrieval
        print(f"{'=' * 100}\nüîç STEP 1: RETRIEVAL\n{'-' * 100}")
        context_chunks = text_processor.search(
            search_mode=SearchMode.COSINE_DISTANCE,
            user_request=user_request,
            top_k=5,
            score_threshold=0.5,
            dimensions=1536
        )
        
        # Step 2: Augmentation
        print(f"\n{'=' * 100}\nüîó STEP 2: AUGMENTATION\n{'-' * 100}")
        context_text = "\n\n".join(context_chunks)
        augmented_prompt = USER_PROMPT.format(context=context_text, query=user_request)
        conversation.add_message(Message(Role.USER, augmented_prompt))
        print(f"Prompt:\n{augmented_prompt}")
        
        # Step 3: Generation
        print(f"\n{'=' * 100}\nü§ñ STEP 3: GENERATION\n{'-' * 100}")
        ai_message = chat_client.get_completion(conversation.get_messages())
        print(f"‚úÖ RESPONSE:\n{ai_message.content}")
        print("=" * 100)
        conversation.add_message(ai_message)


if __name__ == "__main__":
    # PAY ATTENTION THAT YOU NEED TO RUN Postgres DB ON THE 5433 WITH PGVECTOR EXTENSION!
    # RUN docker-compose.yml
    main()